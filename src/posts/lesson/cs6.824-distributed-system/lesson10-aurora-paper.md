---
category: 
  - 分布式系统
tag:
  - 分布式系统
---

- [Spanner：Google 的全球分布式数据库](#spannergoogle-的全球分布式数据库)


# Amazon Aurora：高吞吐量云原生关系数据库的设计考虑

## 摘要

亚马逊 Aurora 是亚马逊网络服务（AWS）提供的一种面向联机事务处理（OLTP）工作负载的关系数据库服务。在本文中，我们将描述 Aurora 的架构以及促成该架构的设计考虑因素。

我们认为，在高吞吐量数据处理中，核心限制已经从**计算和存储**转移到了**网络**上。Aurora 为关系数据库带来了一种新颖的架构以应对这一限制，最显著的是将**重做日志处理**推送到为 Aurora 专门构建的多租户横向扩展存储服务中。

我们描述了这样做如何不仅减少了网络流量，还允许快速的崩溃恢复、在不丢失数据的情况下故障转移到副本以及实现容错、自我修复的存储。然后，我们描述了 Aurora 如何使用高效的异步方案在众多存储节点上就持久状态达成共识，避免了昂贵且频繁通信的恢复协议。最后，在将 Aurora 作为生产服务运行了超过 18 个月后，我们分享了从客户那里学到的关于现代云应用程序对其数据库层的期望的经验教训。

## 1.介绍

随着云服务的广泛推广， 数据库的云服务化也在开展之中。许多客户需要关系型联机事务处理（OLTP）数据库， 他们期望提供与本地部署数据库同等或更优越的功能。

在现代分布式云服务中，**弹性**和**可扩展性**越来越多地通过将**计算**与**存储**解耦以及在多个节点上复制存储来实现。这样做使我们能够处理诸如替换行为异常或无法访问的主机、添加副本、从写入节点故障转移到副本、扩大或缩小数据库实例的规模等操作。

在这种环境下，传统数据库系统面临的 I/O 瓶颈发生了变化。由于 I/O 可以分布在多租户集群中的许多节点和许多磁盘上，单个磁盘和节点不再是热点。相反，瓶颈转移到了请求 I/O 的数据库层与执行这些 I/O 的存储层之间的**网络上**。除了每秒**数据包数**（PPS）和**带宽**的基本瓶颈之外，由于高性能数据库会并行地向存储集群发出写操作，所以流量会被放大。异常的存储节点、磁盘或网络路径的性能可能会主导响应时间。

虽然数据库中的大多数操作可以相互重叠，但有几种情况需要同步操作。这些情况会导致停顿和上下文切换。
- 其中一种情况是由于数据库缓冲缓存未命中而进行的磁盘读取。读取线程在其读取完成之前无法继续。缓存未命中还可能会导致额外的惩罚，即逐出并刷新一个脏缓存页以容纳新页。诸如检查点和脏页写入之类的后台处理可以减少这种惩罚的发生，但也可能会导致停顿、上下文切换和资源争用。
- 事务提交是另一个干扰源；一个事务提交的停顿会阻碍其他事务的进展。在云规模的分布式系统中，使用两阶段提交（2PC）等多阶段同步协议来处理提交是具有挑战性的。这些协议不能容忍故障，而大规模分布式系统持续存在硬故障和软故障的 "背景噪声"。它们的延迟也很高，因为大规模系统分布在多个数据中心。

在本文中，我们描述了亚马逊 Aurora，这是一种新的数据库服务，它通过在高度分布式的云环境中更积极地利用重做日志(redo log)来解决上述问题。我们使用一种新颖的面向服务的架构（见图 1），其中包括一个多租户横向扩展存储服务，该服务抽象出一个虚拟化的分段重做日志，并与一组数据库实例松散耦合。尽管每个实例仍然包含传统内核的大部分组件（查询处理器、事务、锁定、缓冲缓存、访问方法和撤销管理），但有几个功能（重做日志记录、持久存储、崩溃恢复以及备份 / 还原）被卸载到存储服务中。

图1：

我们的架构与传统方法相比有三个显著优势:
- 首先，通过在多个数据中心构建独立的容错且能自我修复的存储服务，我们使数据库免受网络层或存储层的性能波动以及瞬时或永久性故障的影响。
- 其次，通过只向存储写入重做日志记录，我们能够将网络 IOPS（每秒输入 / 输出操作次数）降低一个数量级。一旦我们消除了这个瓶颈，我们就能够积极地优化众多其他的竞争点，从而在我们所基于的 MySQL 代码基础上获得显著的吞吐量提升。
- 第三，我们将一些最复杂和关键的功能（备份和重做恢复）从数据库引擎中一次性的昂贵操作转变为在大型分布式集群中分摊的连续异步操作。这产生了无需检查点的近乎即时的崩溃恢复，以及不干扰前台处理的低成本备份。

在本文中，我们描述了三个贡献：
- 如何在云规模下考虑持久性以及如何设计对相关故障具有弹性的仲裁系统。（第 2 节）。
- 如何通过将传统数据库的底层四分之一部分卸载到智能存储层来利用智能存储。（第 3 节）。
- 如何在分布式存储中消除多阶段同步、崩溃恢复和检查点。（第 4 节）。

然后，我们在第 5 节展示如何将这三个理念结合起来设计 Aurora 的整体架构，接着在第 6 节回顾我们的性能结果，在第 7 节介绍我们所学到的经验教训。最后，我们在第 8 节简要综述相关工作，并在第 9 节给出结论性评论。


## 2.大规模下的持久性

如果一个数据库系统不做其他任何事情，它必须满足这样一个约定：一旦数据被写入，就可以被读取。并非所有系统都能做到这一点。在本节中，我们将讨论我们的仲裁模型背后的基本原理、我们为什么对存储进行分段，以及这两者如何结合起来不仅提供持久性、可用性和减少抖动，还帮助我们解决大规模管理存储集群的操作问题。

### 2.1 复制与相关故障

实例的生命周期与存储的生命周期没有很好的相关性。实例会出现故障。客户会关闭它们。他们会根据负载对其进行大小调整。由于这些原因，将**存储层**与**计算层**解耦是有帮助的。

一旦这样做了，那些存储节点和磁盘也可能会出现故障。因此，它们必须以某种形式进行复制，以提供对故障的弹性。在大规模的云环境中，存在持续的低水平的节点、磁盘和网络路径故障的背景噪声。每个故障可能有不同的持续时间和不同的影响范围。例如，可能会出现对一个节点的网络可用性暂时缺失、重启时的临时停机，或者磁盘、节点、机架、叶节点或骨干网络交换机的永久故障，甚至是数据中心的故障。

在复制系统中容忍故障的一种方法是使用基于仲裁的投票协议。如果复制数据项的 V 个副本中的每一个都被分配一个投票权，那么读操作或写操作必须分别获得 ```Vr``` 个读仲裁投票或 ```Vw``` 个写仲裁投票。为了实现一致性，仲裁必须遵守两个规则。首先，每次读操作必须知道最近的写操作，表述为 ```Vr + Vw > V```。这个规则确保用于读操作的节点集合与用于写操作的节点集合有交集，并且读仲裁包含至少一个具有最新版本的位置。

其次，每次写操作必须知道最近的写操作以避免冲突的写操作，表述为 ```Vw > V/2```。(此话怎么讲？)

一种容忍单个节点丢失的常见方法是将数据复制到（V = 3）个节点，并依赖于 2/3 的写仲裁（Vw = 2）和 2/3 的读仲裁（Vr = 2）。

我们认为 2/3 的仲裁是不够的。为了理解为什么，让我们首先了解一下 AWS 中的可用区（AZ）的概念。一个可用区是一个区域的子集，它通过低延迟链路与该区域中的其他可用区相连，但在大多数故障情况下是隔离的，包括电力、网络、软件部署、洪水等。在多个可用区之间分布数据副本可以确保大规模的典型故障模式只会影响一个数据副本。这意味着可以简单地将三个副本中的每一个放置在不同的可用区中，并且除了较小的单个故障外，还能容忍大规模事件。

然而，在一个大型存储集群中，故障的背景噪声意味着在任何给定的时间点，一些磁盘或节点的子集可能已经出现故障并正在进行修复。这些故障可能独立地分布在可用区 A、B 和 C 的每个节点中。但是，由于火灾、屋顶坍塌、洪水等原因导致可用区 C 出现故障，将打破任何同时在可用区 A 或可用区 B 中出现故障的副本的仲裁。在这种情况下，在 2/3 的读仲裁模型中，我们将丢失两个副本，并且无法确定第三个副本是否是最新的。换句话说，虽然每个可用区中的副本的单个故障是不相关的，但一个可用区的故障是该可用区中所有磁盘和节点的相关故障。仲裁需要容忍可用区故障以及同时发生的背景噪声故障。

在 Aurora 中，我们选择了一个设计点，即容忍（a）丢失整个可用区和一个额外的节点（AZ+1）而不丢失数据，以及（b）丢失整个可用区而不影响写入数据的能力。

我们通过在 3 个可用区中以 6 种方式复制每个数据项来实现这一点，每个可用区中每个数据项有 2 个副本。我们使用具有 6 个投票权（V = 6）、4/6 的写仲裁（Vw = 4）和 3/6 的读仲裁（Vr = 3）的仲裁模型。有了这样的模型，我们可以（a）丢失单个可用区和一个额外的节点（3 个节点的故障）而不会失去读可用性，并且（b）丢失任意两个节点，包括单个可用区故障，并保持写可用性。确保读仲裁使我们能够通过添加额外的副本副本来重建写仲裁。

### 2.2 分段存储

让我们考虑一下 AZ+1 是否提供了足够的持久性的问题。为了在这个模型中提供足够的持久性，必须确保在修复其中一个故障（平均修复时间 ——MTTR）所需的时间内，不相关故障上的双重故障概率（平均故障间隔时间 ——MTTF）足够低。

如果双重故障的概率足够高，我们可能会在可用区故障时看到这些情况，从而打破仲裁。在一定程度之后，很难降低独立故障的平均故障间隔时间（MTTF）的概率。相反，我们专注于减少平均修复时间（MTTR），以缩小易受双重故障影响的时间窗口。我们通过将数据库卷分割成小的固定大小的段来实现这一点，目前段的大小为 10GB。这些段以 6 种方式复制到保护组（PG）中，以便每个 PG 由六个 10GB 的段组成，分布在三个可用区中，每个可用区有两个段。存储卷是一组连接的 PG，在物理上使用大量的存储节点来实现，这些存储节点被配置为带有附加固态硬盘（SSD）的虚拟主机，使用亚马逊弹性计算云（EC2）。组成卷的 PG 随着卷的增长而分配。我们目前支持在未复制的基础上可以增长到 64TB 的卷。

现在，段是我们独立的背景噪声故障和修复的单位。我们作为服务的一部分对故障进行监控并自动修复。在 10Gbps 的网络链路上，一个 10GB 的段可以在 10 秒内修复。我们需要在同一个 10 秒窗口内看到两次这样的故障，再加上一个不包含这两个独立故障中任何一个的可用区的故障，才会失去仲裁。按照我们观察到的故障发生率，这种情况极不可能发生，即使对于我们为客户管理的数据库数量来说也是如此。

### 2.3 弹性的操作优势
一旦设计出一个对长时间故障具有天然弹性的系统，它自然也对较短时间的故障具有弹性。一个能够处理可用区长期丢失的存储系统也能够处理由于电力事件或需要回滚的不良软件部署而导致的短暂中断。一个能够处理仲裁成员多秒可用性丢失的系统能够处理网络拥塞或存储节点上的短暂负载时期。