---
category: 
  - 分布式系统
tag:
  - 分布式系统
---

- [Spanner：Google 的全球分布式数据库](#spannergoogle-的全球分布式数据库)
  - [摘要](#摘要)
  - [1.介绍](#1介绍)
  - [2.实现](#2实现)
    - [2.1 Spanner 服务器软件栈](#21-spanner-服务器软件栈)
    - [2.2 目录与放置](#22-目录与放置)
    - [2.3 数据模型](#23-数据模型)

# Spanner：Google 的全球分布式数据库

## 摘要

Spanner是谷歌的可伸缩，多版本，全球化分布，同步多副本的数据库。它是首个能将数据分布到全球范围内且支持外部一致性的分布式事务的系统。本文介绍了如下几个方面：
- Spanner的结构、特性、底层设计决策的原理
- 公开时钟不确定性的新型时间API。此 API 及其实现对于支持外部一致性以及各种强大功能至关重要：在整个 Spanner 中实现对过去的无阻塞读取、无锁只读事务以及原子模式更改。

注：Spanner底层并不是纯粹的关系型数据库，但是提供了SQL查询语言。

## 1.介绍

Spanner 是一种可扩展的、全球分布式数据库，由谷歌设计、构建并部署。在最高抽象层次上，它是一个在遍布全球的数据中心中的多组 Paxos 状态机上对数据进行分片的数据库。复制用于实现全局可用性和地理局部性；客户端会在副本之间自动进行故障转移。随着数据量或服务器数量的变化，Spanner 会自动在机器之间重新分片数据，并且它会自动在机器之间（甚至跨数据中心）迁移数据，以平衡负载并应对故障。Spanner 被设计为可扩展到数百个数据中心的数百万台机器以及数万亿条数据库行。

通过在一个大洲内部甚至跨大洲复制其数据，应用程序可以使用 Spanner 实现高可用性，即使在面临广域自然灾害的情况下也是如此。我们的最初客户是 F1，它是谷歌广告后端的重写版本。F1 使用分布在美国各地的五个副本。F1 使用分布在美国各地的五个副本。大多数其他应用程序可能会在一个地理区域的三到五个数据中心之间复制其数据，但具有相对独立的故障模式。也就是说，大多数应用程序只要能够在一两个数据中心出现故障的情况下存活下来，就会选择较低的延迟而不是较高的可用性。(副本数量越多，可用性越强，但是延迟越高)。

Spanner 的主要重点是管理跨数据中心的复制数据，但我们也在分布式系统基础设施之上设计和实现重要的数据库功能方面花费了大量时间。

尽管许多项目愉快地使用 Bigtable，但我们也一直收到用户的抱怨，即对于某些类型的应用程序，Bigtable 可能难以使用。
- 那些具有复杂、不断演变的模式的应用，或者那些在广域复制的情况下想要强一致性的应用。（其他作者也提出了类似的主张)

谷歌的许多应用程序选择使用，是因为它的半关系型数据模型以及对同步复制的支持，尽管它的写入吞吐量相对较低。

因此，Spanner 已从类似 Bigtable 的带版本控制的键值存储演变为一个时间多版本数据库。数据存储在有模式的半关系型表中；数据是有版本的，并且每个版本都自动带有其提交时间的时间戳；旧版本的数据受可配置的垃圾回收策略约束；并且应用程序可以读取旧时间戳下的数据。Spanner 支持通用事务，并提供一种基于 SQL 的查询语言。

作为一个全球分布式数据库，Spanner 提供了一些有趣的特性。
- 首先，应用程序可以对数据的复制配置进行精细的动态控制。应用程序可以指定约束条件来控制哪些数据中心包含哪些数据、数据与用户的距离（以控制读取延迟）、副本之间的距离（以控制写入延迟）以及维护的副本数量（以控制持久性、可用性和读取性能）。数据也可以由系统在数据中心之间动态且透明地移动，以平衡跨数据中心的资源使用。
- 第二，Spanner 有两个在分布式数据库中难以实现的特性：它提供外部一致的读和写，以及在某个时间戳下整个数据库的全局一致读。这些特性使 Spanner 能够在全球范围内支持一致的备份、一致的 MapReduce 执行和原子模式更新，即使在有正在进行的事务的情况下也是如此。

这些特性是由这样一个事实实现的，即尽管事务可能是分布式的，但 Spanner 为事务分配具有全局意义的提交时间戳。这些时间戳反映了序列化顺序。此外，序列化顺序满足外部一致性（或者等价地，满足线性一致性）：如果一个事务 T1 在另一个事务 T2 开始之前提交，那么 T1 的提交时间戳小于 T2 的提交时间戳。Spanner 是第一个在全球范围内提供此类保证的系统。

这些特性的关键促成因素是新的 TrueTime API 及其实现。该 API 直接暴露时钟不确定性，而 Spanner 的时间戳保证取决于实现所提供的界限。如果不确定性很大，Spanner 会放慢速度以等待这种不确定性消失。谷歌的集群管理软件提供了 TrueTime API 的一种实现。这种实现通过使用多个现代时钟参考（GPS 和原子钟）使不确定性保持在较小范围内（通常小于 10 毫秒）。

论文的后续安排如下所示：
- 第 2 节描述了 Spanner 实现的结构、其功能集以及在设计中所做的工程决策。
- 第 3 节描述了我们新的 TrueTime API 并概述了其实现。
- 第 4 节描述了 Spanner 如何使用 TrueTime 来实现外部一致的分布式事务、无锁只读事务和原子模式更新。
- 第 5 节提供了一些关于 Spanner 性能和 TrueTime 行为的基准测试，并讨论了 F1 的经验。
- 第 6、7 和 8 节描述了相关工作和未来工作，并总结了我们的结论。

## 2.实现

本节描述了 Spanner 实现的结构及其背后的基本原理。然后描述了目录抽象，它用于管理复制和局部性，并且是数据移动的单位。最后，描述了我们的数据模型，为什么 Spanner 看起来像一个关系型数据库而不是键值存储，以及应用程序如何控制数据局部性。

一个 Spanner 部署被称为一个 universe。鉴于 Spanner 在全球范围内管理数据，运行中的 universe 只会有几个。我们目前运行着一个测试 / 试验 universe、一个开发 / 生产 universe 以及一个仅用于生产的 universe。

Spanner 被组织为一组 zone，其中每个zone大致类似于一个 Bigtable 服务器的部署。zone是管理部署的单位。zone集合也是数据可以被复制的位置集合。随着新的数据中心投入使用和旧的数据中心关闭，区域可以分别被添加到正在运行的系统中或从其中移除。Zone也是物理隔离的单位：例如，在一个数据中心中可能有一个或多个Zone，如果不同应用程序的数据必须在同一个数据中心的不同服务器集合之间进行分区的话。

![Spanner服务器状态](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/lesson13/paper/fig1-spanner-server-organization.png)

图 1 展示了 Spanner universe 中的服务器。一个zone有一个zonemaster以及在一百到几千个 Spanner server之间。前者将数据分配给 Spanner server；后者为客户端提供数据服务。每个zone的location proxy被客户端用来定位被分配为其提供数据服务的 Spanner server。

目前，universe master和placement driver都是单一的实例。universe主服务器主要是一个控制台，用于显示所有区域的状态信息，以便进行交互式调试。placement driver在几分钟的时间尺度上处理数据在不同区域之间的自动移动。placement driver会定期与 Spanner 服务器通信，以找到需要移动的数据，要么是为了满足更新后的复制约束，要么是为了平衡负载。由于篇幅原因，我们将只详细描述 Spanner 服务器。

### 2.1 Spanner 服务器软件栈

本节重点介绍 Spanner 服务器的实现，以说明复制和分布式事务是如何分层构建在我们基于 Bigtable 的实现之上的。软件栈如图 2 所示。在底层，每个 Spanner 服务器负责 100 到 1000 个被称为 "数据片"（tablet）的数据结构实例。一个数据片类似于 Bigtable 的数据片抽象，因为它实现了一组如下的映射：

```shell
(key: string, timestamp: int64) -> string
```

与 Bigtable 不同，Spanner 为数据分配时间戳，这是 Spanner 更像一个多版本数据库而不是键值存储的一个重要方式。一个数据片的状态存储在一组类似 B 树的文件和一个预写日志中，所有这些都存储在一个名为 Colossus 的分布式文件系统中（它是 Google 文件系统的后续版本）。

为了支持复制，每个 Spanner 服务器在每个数据片之上实现一个单独的 Paxos 状态机。（早期的 Spanner 版本在每个数据片上支持多个 Paxos 状态机，这允许更灵活的复制配置。但该设计的复杂性使我们放弃了它。）每个状态机将其元数据和日志存储在相应的数据片中。我们的 Paxos 实现支持具有基于时间的领导权租约的长期领导者，其租约时长默认为 10 秒。当前的 Spanner 实现将每个 Paxos 写入操作记录两次：一次在数据片的日志中，一次在 Paxos 日志中。这个选择是出于权宜之计，我们最终可能会纠正这一点。我们对 Paxos 的实现是流水线式的，以便在存在广域网延迟的情况下提高 Spanner 的吞吐量；但是 Paxos 会按顺序应用写入操作（在第 4 节中我们将依赖这一事实）。

Paxos 状态机被用来实现一个一致复制的映射集合。每个副本的键值映射状态存储在其相应的数据片中。写入操作必须在领导者处启动 Paxos 协议；读取操作可以从任何足够新的副本的底层数据片中直接访问状态。副本集合共同构成一个 Paxos 组。

在每个作为领导者的副本处，每个 Spanner 服务器实现一个锁表来实现并发控制。锁表包含两阶段锁的状态：它将键的范围映射到锁状态。（注意，拥有一个长期存在的 Paxos 领导者对于高效管理锁表至关重要。）在 Bigtable 和 Spanner 中，我们都为长期事务（例如，可能需要几分钟的报告生成）进行了设计，在存在冲突的情况下，这些事务在乐观并发控制下表现不佳。需要同步的操作，如事务性读取，在锁表中获取锁；其他操作绕过锁表。

在每个作为领导者的副本处，每个 Spanner 服务器还实现一个事务管理器来支持分布式事务。事务管理器用于实现一个参与者领导者；组中的其他副本将被称为参与者从属。如果一个事务只涉及一个 Paxos 组（大多数事务都是如此），它可以绕过事务管理器，因为锁表和 Paxos 一起提供了事务性。如果一个事务涉及多个 Paxos 组，这些组的领导者进行协调以执行两阶段提交。其中一个参与者组被选为协调者：该组的参与者领导者将被称为协调者领导者，该组的从属将被称为协调者从属。每个事务管理器的状态存储在底层的 Paxos 组中（因此是被复制的）。

### 2.2 目录与放置

在键值映射集合之上，Spanner 实现支持一种称为 "目录"（directory）的分桶抽象，它是一组具有共同前缀的连续键的集合。（"目录" 这个术语的选择是一个历史偶然；一个更好的术语可能是 "桶"（bucket）。）我们将在 2.3 节中解释这个前缀的来源。支持目录使得应用程序能够通过谨慎选择键来控制其数据的局部性。

目录是数据放置的单位。一个目录中的所有数据具有相同的复制配置。当数据在 Paxos 组之间移动时，它是按目录逐个移动的，如图 3 所示。Spanner 可能会移动一个目录以减轻一个 Paxos 组的负载；将经常被一起访问的目录放入同一个组中；或者将一个目录移动到更接近其访问者的组中。在客户端操作进行时可以移动目录。可以预期一个 50MB 的目录可以在几秒钟内被移动。

一个 Paxos 组可能包含多个目录这一事实意味着 Spanner 的数据片与 Bigtable 的数据片不同：前者不一定是行空间的单个按字典序连续的分区。相反，Spanner 的数据片是一个容器，可以封装行空间的多个分区。我们做出这个决定是为了能够将经常一起被访问的多个目录放置在同一位置。

"Movedir" 是用于在 Paxos 组之间移动目录的后台任务。"Movedir" 也被用于向 Paxos 组添加或移除副本，因为 Spanner 尚不支持在 Paxos 内部进行配置更改。"Movedir" 不是作为单个事务来实现的，以避免在大量数据移动时阻塞正在进行的读取和写入操作。相反，"Movedir" 记录它开始移动数据的事实，并在后台移动数据。当它移动了除了少量标称数量的数据之外的所有数据时，它使用一个事务来原子性地移动该标称数量的数据，并更新两个 Paxos 组的元数据。

目录也是应用程序可以指定其地理复制属性（简称为放置）的最小单位。我们的放置规范语言的设计将管理复制配置的职责分开。管理员控制两个维度：副本的数量和类型，以及这些副本的地理位置放置。他们在这两个维度上创建一个命名选项的菜单（例如，北美，以五种方式复制并带有一个见证者）。应用程序通过用这些选项的组合标记每个数据库和 / 或单个目录来控制数据的复制方式。例如，应用程序可以将每个最终用户的数据存储在其自己的目录中，这将使得用户 A 的数据在欧洲有三个副本，而用户 B 的数据在北美有五个副本。

为了阐述清晰，我们进行了过度简化。实际上，如果一个目录变得太大，Spanner 会将其分割成多个片段。片段可以从不同的 Paxos 组（因此也是不同的服务器）提供服务。“Movedir” 实际上在组之间移动的是片段，而不是整个目录。

### 2.3 数据模型

Spanner 向应用程序公开了以下一组数据特性：基于模式化半关系表的数据模型、一种查询语言以及通用事务。朝着支持这些特性的方向发展是由许多因素驱动的。支持模式化半关系表和同步复制的需求是由 Megastore 的流行所推动的。谷歌内部至少有 300 个应用程序使用 Megastore（尽管它的性能相对较低），因为它的数据模型比 Bigtable 的更容易管理，并且因为它支持跨数据中心的同步复制。(Bigtable 只支持跨数据中心的最终一致性复制。)使用 Megastore 的著名谷歌应用程序的例子有 Gmail、Picasa、日历、Android 市场和 App Engine。考虑到 Dremel 作为一种交互式数据分析工具的流行，Spanner 中支持类似 SQL 的查询语言的需求也很明确。最后，Bigtable 中缺乏跨行事务导致了频繁的抱怨；Percolator 在一定程度上是为了解决这个缺陷而构建的。一些作者声称，通用的两阶段提交由于其带来的性能或可用性问题而过于昂贵而难以支持。我们认为，最好让应用程序程序员在出现瓶颈时处理由于过度使用事务而导致的性能问题，而不是总是围绕缺乏事务进行编码。在 Paxos 上运行两阶段提交缓解了可用性问题。

应用程序的数据模型构建在实现所支持的基于目录分桶的键值映射之上。一个应用程序在一个 "全域" 中创建一个或多个数据库。每个数据库可以包含无限数量的模式化表。表看起来像关系数据库表，具有行、列和带版本的值。我们不会详细介绍 Spanner 的查询语言。它看起来像 SQL，并带有一些扩展以支持协议缓冲区（protocol buffer）值的字段。

图 4 包含了一个在每个用户、每张专辑的基础上存储照片元数据的 Spanner 模式示例。模式语言与 Megastore 的类似，另外要求每个 Spanner 数据库必须由客户端划分成一个或多个表的层次结构。客户端应用程序通过 "INTERLEAVE IN" 声明在数据库模式中声明层次结构。层次结构顶部的表是一个目录表。目录表中具有键 K 的每一行，以及按字典顺序以 K 开头的所有后代表中的行，共同形成一个目录。"ON DELETE CASCADE" 表示删除目录表中的一行将删除任何相关的子行。该图还展示了示例数据库的交错布局：例如，"Albums (2,1)" 表示用户 ID 为 2、专辑 ID 为 1 的 "Albums" 表中的一行。这种表的交错形成目录是很重要的，因为它允许客户端描述多个表之间存在的局部性关系，这对于分片的分布式数据库中的良好性能是必要的。如果没有它，Spanner 将不知道最重要的局部性关系。


https://blog.mrcroxx.com/posts/paper-reading/spanner-osdi2012/#2-%E5%AE%9E%E7%8E%B0