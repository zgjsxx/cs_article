---
category: 
  - 分布式系统
tag:
  - 分布式系统
---

- [raft算法](#raft算法)
  - [3. Paxos的问题](#3-paxos的问题)
  - [4. 面向可理解性的设计](#4-面向可理解性的设计)
  - [5. raft一致性算法](#5-raft一致性算法)
    - [5.1 raft算法基础](#51-raft算法基础)
      - [5.1.1 raft状态机](#511-raft状态机)
      - [5.1.2 raft任期](#512-raft任期)
      - [5.1.3 raft节点通讯](#513-raft节点通讯)
    - [5.2 Leader选举](#52-leader选举)
      - [5.2.1 选举过程](#521-选举过程)
      - [5.2.2 选举成功的条件](#522-选举成功的条件)
    - [5.2.3 如何避免无限循环的投票分裂](#523-如何避免无限循环的投票分裂)
    - [5.3 日志复制](#53-日志复制)
      - [5.3.1 复制的流程](#531-复制的流程)
      - [5.3.2 日志的组织方式](#532-日志的组织方式)
      - [5.3.3 提交日志](#533-提交日志)
      - [5.3.4 匹配日志](#534-匹配日志)
    - [5.3.5 日志不一致的场景](#535-日志不一致的场景)
      - [5.3.6 避免 log 不一致：AppendEntries 中的一致性检查](#536-避免-log-不一致appendentries-中的一致性检查)
    - [5.4 安全性](#54-安全性)
      - [5.4.1 限制一：包含所有已提交 entry 的节点才能被选为 leader](#541-限制一包含所有已提交-entry-的节点才能被选为-leader)
      - [5.4.2 限制二：](#542-限制二)
      - [5.4.3 安全性论证](#543-安全性论证)
    - [5.5 Follower/candidate 故障](#55-followercandidate-故障)
    - [5.6 时序](#56-时序)
  - [问题](#问题)
    - [raft算法，有没有可能有多个candidate同时获得了多数票？](#raft算法有没有可能有多个candidate同时获得了多数票)
    - [raft算法中，节点根据什么规则投票?](#raft算法中节点根据什么规则投票)
  - [资源](#资源)

# raft算法

## 3. Paxos的问题

在过去十年间，莱斯利・兰伯特（Leslie Lamport）的 **Paxos 协议**几乎成了**共识算法**（consensus）的代名词：它是课程中最常讲授的协议，而且大多数共识实现都将其作为起始点。

Paxos 首先定义了一个能够就单个决策（例如单个复制日志条目）达成一致的协议。我们将这个子集称为单决议 Paxos（single - decree Paxos）。然后，Paxos 组合该协议的多个实例以促成一系列决策，例如日志（多 Paxos，multi - Paxos）。Paxos 确保安全性和活性（liveness），并且支持集群成员关系的变更。其正确性已得到证明，而且在正常情况下是高效的。

Paxos算法的缺点如下：
- 第一个缺点是 Paxos 是出了名的难以理解。
- 第二个缺点没有考虑真实系统的实现，很难用于实际系统。

基于上述问题，作者认为Paxos既不能为开发真实系统提供良好的基础，又不适用于教学。而考虑到大型软件系统中共识的重要性，我们决定自己设计一种替代 Paxos、有更好特性的共识算法。Raft 就是这一实验的产物。

## 4. 面向可理解性的设计

设计 Raft 时我们有几个目标：

- 必须为构建真实系统提供完整基础，能显著降低系统开发者所需的设计工作；
- 必须在所有情况下确保安全（不会导致冲突），在典型场景下确保可用；
- 对于常见操作必须很高效，
- 必须确保可理解性（understandability），这才是我们最重要的目标，同时也是最大的挑战。

## 5. raft一致性算法

### 5.1 raft算法基础

#### 5.1.1 raft状态机

一个 Raft 集群包含若干个服务器；通常有五个，这使得系统可以容忍两个节点发生故障。

在任何特定时间，每个服务器都处于三种状态之一：**领导者**（leader）、**跟随者**（follower）或者**候选者**（candidate）。

- 在正常运行时，只有**一个领导者**，其他所有服务器**都是跟随者**。
- 跟随者是被动的，它们自身不发出请求，而只是响应来自领导者和候选者的请求。
- 领导者处理所有客户端请求， 如果客户端与一个跟随者联系，跟随者会将**其重定向到领导者**。
- 候选者是一个特殊状态，用于选举新的领导者。

下图展示了这些状态及其转换：

![raft服务器状态](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/raft-server-state.png)

#### 5.1.2 raft任期

下一个要介绍的概念是**任期**。Raft 算法将时间划分为任意长度的任期，任期用**连续的整数编号**。每个任期都以一次选举开始，在选举中，一个或多个候选人尝试成为领导者。如果一个候选人在选举中获胜，那么它将在该任期的剩余时间里担任领导者。在某些情况下，选举会导致选票分散。在这种情况下，该任期将结束且没有领导者；一个新的任期（伴随着新的选举）很快就会开始。Raft 算法确保在一个特定任期内最多只有一个领导者。

![raft任期](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/raft-term.png)

关于raft，有以下几点需要说明：
- 每个任期的开始都是从**选举**开始，选举时可能有多个都试图成为leader。
- 某个candidate赢得选举之后，就会成为该任期内的leader。
- 每个节点都记录了当前的**任期号**（currentTerm），这个编号时随着时间单调递增。
- 节点通信时，会带上自己的任期号
  - 如果自己的任期号小于其他节点的，要立即将自己的任期号更新为更大的任期号。
  - 如果一个candidate或leader发现自己的任期过期了，要立即切换到follower状态。
  - 如果一个节点收到了携带过期任期编号的请求，会拒绝这个请求。

#### 5.1.3 raft节点通讯

raft服务器RPC进行通讯，主要通讯交互是两种：

- RequestVote： 由candidates在选举期间发起
- AppendEntries： 由leader发起，用于复制日志条目以及提供一种心跳（heartbeat）形式。

### 5.2 Leader选举

Raft 使用**心跳机制**来触发领导者选举。
- 当服务器启动时，它们以跟随者（follower）状态开始。只要服务器从领导者或候选人那里接收到有效的远程过程调用（RPC），它就会保持在跟随者状态。
- 领导者会定期向所有跟随者发送心跳（空的AppendEntries消息）以维护其权威性。
- 如果一个跟随者在一段称为选举超时（election timeout）的时间内没有收到任何通信，那么它会假定没有有效的领导者，并开始进行选举以选出新的领导者。

#### 5.2.1 选举过程

对于一个follower，当其发生选举超时时，将开始选举，具体过程如下
- 增加它的当前任期
- 将状态机切换到candidate状态。
- 然后选举自己作为leader，同时并发地向集群其他节点发送RequestVote RPC

对于该follower，其选举结果可能由如下三种：
- 该follower赢得此次选举，成为leader
- 另一个节点赢得此次选举，成为leader
- 选举超时，没有产生有效leader

#### 5.2.2 选举成功的条件

如果候选人在同一任期内获得了整个集群中**大多数服务器**的选票，它就赢得了选举。在给定的任期内，每个服务器最多为一个候选人投票，遵循**先到先得**的原则。多数原则确保了在特定任期内最多只有一个候选人能够赢得选举。一旦候选人赢得选举，它就成为领导者。然后它向所有其他服务器发送心跳消息以确立其权威并防止新的选举。

在等待投票期间，一个 candidate 可能会从其他服务器收到一个 AppendEntries RPC 声称自己是 leader。这个 leader 的任期 term 包含在 RPC 消息中，根据term值，分两种情况处理：

 - 该term大于等于这个 candidate 的 currentTerm：那该 candidate 就承认 这个 leader 是合法的，然后回归到 follower 状态。
 - 该term小于这个 candidate 的 currentTerm：拒绝这个 RPC ，仍然留在 candidate 状态。

第 3 种可能的结果是：该 candidate 既没有赢得也没有输掉这次选举。 如果多个 followers 在同一时间成为 candidates，投票就会很分散，最终没有谁能赢得大多数选票。 当发生这种情况时，每个 candidate 都会超时，然后各自增大 term 并给其他节点发送 RequestVote 请求，开始一轮新选举， 但如果没有额外的预防措施，**这种投票分裂的情况看可能会无限持续下去**。

### 5.2.3 如何避免无限循环的投票分裂

Raft 使用**随机化的选举超时时间**来确保选票分散（split votes）的情况很少发生并且能够被快速解决。

为了从一开始就防止选票分散的情况，选举超时时间是从一个固定区间（例如，150 - 300 毫秒）中随机选择的。这样可以分散服务器的超时时间，使得在大多数情况下只有一台服务器会超时；这台服务器赢得选举并在其他服务器超时之前发送心跳信息。同样的机制也被用于处理选票分散的情况。每个候选人在选举开始时重新启动其随机化的选举超时时间，并在该超时时间结束后才开始下一次选举；这降低了在新选举中再次出现选票分散的可能性。9.3 节表明这种方法可以快速选出领导者。

作者这里提到了曾经设计过使用排名系统来解决选票分散问题，即每个候选人被分配一个唯一的排名，该排名用于在相互竞争的候选人之间进行选择。如果一个候选人发现另一个具有更高排名的候选人，它将返回跟随者状态，以便更高排名的候选人可以更容易地赢得下一次选举。但是这种方法在可用性方面产生了一些微妙的问题。如果高排名的服务器出现故障，低排名的服务器可能需要超时并再次成为候选人，但如果它太快这样做，可能会重置选举领导者的进度）。我们对该算法进行了多次调整，但每次调整后都会出现新的极端情况。最终我们得出结论，随机重试方法更加直观和易于理解。

综上所述，作者选择了使用**随机化选举超时时间**解决选票分散问题。

### 5.3 日志复制

#### 5.3.1 复制的流程

一旦选举出领导者，它就开始处理客户端请求。每个客户端请求都包含一个要由复制状态机执行的命令。领导者将该命令作为新条目追加到其日志中，然后并行地向其他每个服务器发出附加条目（AppendEntries）RPC 以复制该条目。当该条目已被安全复制（如下所述）时，领导者将该条目应用于其状态机，并将该执行结果返回给客户端。如果跟随者崩溃或运行缓慢，或者网络数据包丢失，领导者会无限期地重试附加条目 RPC（即使在它已经响应客户端之后），直到所有跟随者最终存储所有日志条目。

#### 5.3.2 日志的组织方式

日志的组织方式下图所示。每个日志条目存储一个状态机命令，以及该条目被领导者接收时的任期编号。日志条目中的任期编号用于检测日志之间的不一致性。每个日志条目还拥有一个整数索引，用于标识其在日志中的位置。

![raft日志项](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/raft-logs-entries.png)

#### 5.3.3 提交日志

Raft 协议保证已提交的条目是持久化的，并且最终将由所有可用的状态机执行。

日志可以被提交的前提是该日志已经被复制到大多数服务器上(半数以上)。

5.4节会讨论leader变更之后应用这个规则时的情况，那时将会看到这种对于 commit 的定义也是安全的。

#### 5.3.4 匹配日志

如果不同 log 中的两个 entry 有完全相同的 index 和 term， 那么意味着：
- 这两个 entry 一定包含了相同的命令
- 在所有前面的条目中这些日志都是相同的。


### 5.3.5 日志不一致的场景

正常情况下，leader 和 follower 的 log 能保持一致，但 leader 挂掉会导致 log 不一致 （leader 还未将其 log 中的 entry 都复制到其他节点就挂了）。 这些不一致会导致一系列复杂的 leader 和 follower crash。 Figure 7 展示了 follower log 与新的 leader log 的几种可能不同：

![raft日志不一致的场景](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/raft-log-not-consistent-case.png)

在上图中：
- 每个方框代表一个日志项
- a-b代表follower缺失了部分日志
- c-d代表follower产生了额外的未提交日志
- e-f既缺失日志又产生了未提交日志

上图中f的场景可能是这样的：服务器是**任期2**的领导者，在其日志中添加了若干条目，然后在提交任何条目之前崩溃；它迅速重启，成为**任期3**的领导者，并在其日志中又添加了一些条目；在任期2或任期3的任何条目被提交之前，该服务器再次崩溃，并在之后的几个任期内一直处于宕机状态。

#### 5.3.6 避免 log 不一致：AppendEntries 中的一致性检查

Raft 处理不一致的方式是强制 follower 复制一份 leader 的 log， 这意味着 follower log 中冲突的 entry 会被 leader log 中的 entry 覆盖。 Section 5.4 将会看到再加上另一个限制条件后，这个日志复制机制就是安全的。

解决冲突的具体流程：

- 找到 leader 和 follower 的最后一个共同认可的 entry，
- 将 follower log 中从这条 entry 开始往后的 entries 全部删掉，
- 将 leader log 中从这条记录开始往后的所有 entries 同步给 follower

### 5.4 安全性

#### 5.4.1 限制一：包含所有已提交 entry 的节点才能被选为 leader

在任何基于 leader 的共识算法中，leader 最终都必须**存储了所有的已提交 entry**。

在某些共识算法中，例如**Viewstamped Replication**，一个节点即使并未包含全部的 已提交 entries 也仍然能被选为 leader。这些算法有特殊的机制来识别缺失 entries， 并在选举期间或选举结束后立即发送给新 leader。不幸的是，这会导致额外的复杂性。

Raft 采取了一种更简单的方式：除非前面所有 term 内的已提交 entry 都已经在某个节点上了，否则这个节点不能被选为 leader（后面将介绍如何保证这一点）。 这意味着无需从 non-leader 节点向 leader 节点同步数据，换句话说 log entries 只会从 leader 到 follower 单向流动。

那这个是怎么做到的呢？通过投票过程。

- 首先，刚才已经提到，除非 log 中已经包含了集群的所有已提交 entries，否则一个 candidate 是不能被选为 leader 的。
- 其次，还活着的（即参与选举的）节点中，至少有一个节点保存了集群的所有已提交 entries （因为覆盖大多数节点的 entry，才算是提交成功的）。
- 那么，只要一个 candidate 的 log 与大多数节点相比至少不落后（at least as up-to-date，这个词下文会有精确定义），那它就持有了集群的所有已提交记录。

因此，只要能确保这里提到的"至少不落后"语意，就能确保选出来的 leader 拥有集群的所有已提交 entries。 RequestVote RPC 实现了这个过程：请求中包含了发送方的 log 信息，如果当前 节点自己的 log 比对方的更新，会拒绝对方成为 leader 的请求。具体到实现上， 判断哪个 log 更加新，依据的是最后一个 entry 的 index 和 term：

- 如果 term 不同，那 term 新的那个 log 胜出；
- 如果 term 相同，那 index 更大（即更长）的那个 log 胜出。

#### 5.4.2 限制二：

下面是一个时间序列展示了为什么领导者不能使用旧任期的日志条目来确定提交状态。

![leader节点不能直接提交旧的任期的日志](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/safety-commit.png)

对于该图的解释如下：

- 在图(a)中, S1是当前的leader，将index=2的日志复制到了S2。
- 在图(b)中, S1挂了，S5被S3/S4/S5选举为新的leader，任期为3，然后S5在index=2的位置接受了一个来自客户端的新的entry。
- 在图(c)中, S1又被选举为下一任leader，任期为4。然后S1继续同步挂掉之前的那个entry(index=2)，它被成功同步到大部分节点上(S1/S2/S3)，但还未提交；

接下来，分两种情况：
- 在图(d)中，如果此时S1再此崩溃，S5可能会被选举为领导者(来自S2，S3，S4，S5的选票)，那么S5会用自己在任期3的index=2的日志覆盖其他节点上的日志。试想对于这种场景，倘若在图(c)中提交日志<2，2>，那么可能存在提交日志被覆盖的场景，这是不可以被接受的。
- 在图(e)中，如果S1没有崩溃，客户端又提交了一个新的entry，并且复制到了大多数服务器上，则此刻可以提交。因为这个场景下，S5没有可能成为leader。

#### 5.4.3 安全性论证

鉴于完整的 Raft 算法，我们现在可以更精确地论证领导者完备性属性成立（这个论证基于安全性证明；见第 9.2 节）。我们假设领导者完备性属性不成立，然后证明一个矛盾。

假设任期 T 的领导者（leaderT）提交了一个来自其任期的日志条目，但该日志条目没有被某个未来任期的领导者存储。考虑最小的任期 U＞T，其领导者（leaderU）没有存储该条目。

- 1. 已提交的条目在 leaderU 被选举时一定不在其日志中（领导者永远不会删除或覆盖条目）。

- 2. leaderT 将该条目复制到了集群中的大多数服务器上，并且 leaderU 从集群中的大多数服务器获得了选票。因此，至少有一个服务器（"投票者"）既接受了来自 leaderT 的条目，又为 leaderU 投了票，如图 9 所示。这个投票者是得出矛盾的关键。

- 3. 投票者一定是在为 leaderU 投票之前就接受了来自 leaderT 的已提交条目；否则它会拒绝来自 leaderT 的附加条目请求（它的当前任期会比 T 更高）。

- 4. 当投票者为 leaderU 投票时，它仍然存储着该条目，因为每一个介于其间的领导者都包含该条目（根据假设），领导者永远不会删除条目，而跟随者只有在条目与领导者冲突时才会删除条目。

- 5. 投票者将其选票投给了 leaderU，所以 leaderU 的日志一定与投票者的日志一样是最新的。这会导致两种矛盾情况之一。

- 6. 首先，如果投票者和 leaderU 具有相同的最后一个日志任期，那么 leaderU 的日志长度一定至少与投票者的日志长度相同，所以它的日志包含投票者日志中的每一个条目。这是一个矛盾，因为投票者包含已提交的条目，而假设 leaderU 不包含。

- 7. 否则，leaderU 的最后一个日志任期一定大于投票者的。此外，它一定大于 T，因为投票者的最后一个日志任期至少是 T（它包含来自任期 T 的已提交条目）。创建 leaderU 的最后一个日志条目的早期领导者的日志中一定包含已提交的条目（根据假设）。然后，根据日志匹配属性，leaderU 的日志也必须包含已提交的条目，这是一个矛盾。

- 8. 这就完成了矛盾的证明。因此，所有大于 T 任期的领导者都必须包含在任期 T 中已提交的来自任期 T 的所有条目。

- 9. 日志匹配属性保证了未来的领导者也将包含间接提交的条目，例如图 8 (d) 中的索引 2。

![必有一个节点既接受append日志又接受Leader投票](https://github.com/zgjsxx/static-img-repo/raw/main/blog/lesson/6.824/safey-argument.png)

### 5.5 Follower/candidate 故障

到目前为止，我们一直关注领导者故障。跟随者和候选者的崩溃比领导者崩溃更容易处理，并且它们都以相同的方式处理。如果一个跟随者或候选者崩溃，那么未来发送给它的请求投票和附加条目 RPC 将失败。Raft 通过无限期重试来处理这些故障；如果崩溃的服务器重新启动，那么 RPC 将成功完成。如果一个服务器在完成 RPC 后但在响应之前崩溃，那么它在重新启动后将再次收到相同的 RPC。Raft RPC 是幂等的，所以这不会造成任何危害。例如，如果一个跟随者收到一个附加条目请求，其中包括已经存在于其日志中的日志条目，它会在新请求中忽略这些条目。


### 5.6 时序

Raft中最依赖时序的就是leader的选举。只要系统满足如下时序条件，Raft就一定可以选出一个稳定的leader：

$$broadcastTime < electionTimeout < MTBF $$

- broadcastTime：一个节点并发给其他节点发送请求并收到响应的平均时间；
- electionTimeout： 5.2 小节定义的选举超时时间；
- MTBF：单个节点的平均故障时间（average time between failures）

这个不等式表达的意思很好理解：

- 广播耗时要比选举超时低一个数量级，这样 leader 才能可靠地发送心跳消息给 follower，避免它们发起来新的选举。 考虑到选举超时是随机化的，这个不等式也使得投票分裂不太可能发生。
- 选举超时要比 MTBF 低几个数量级，这样系统才能稳步前进。 当 leader 挂掉后，系统大致会经历一个 election timeout 时间段的不可用，我们希望这个时间段只占到总时间段的很小一部分。

broadcastTime 和 MTBF 都是底层系统的特性，而 electionTimeout 是我们需要设置的。

- Raft 一般要求接收方将请求持久化到稳定存储中，因此取决于存储技术，broadcastTime 可能需要 0.5ms ~ 20ms。
- 因此，electionTimeout 通常选择 10ms ~ 500ms。
- 典型的节点 MTBF 是几个月或更长时间，因此很容易满足时序要求。

## 问题

### raft算法，有没有可能有多个candidate同时获得了多数票？

在 Raft 共识算法中，多个 candidate 节点同时获得多数票的情况是不可能的。

主要有两个原因：

- 单一投票规则

在每一轮选举（即每个任期 term）中，每个节点只能投票给一个 candidate。因此，一个节点不可能同时把票投给两个不同的 candidate。这意味着，多个 candidate 要同时获得多数票，在逻辑上是互斥的。

- 选举时间随机化

每个节点的 Election Timeout 是随机的，因此不同节点转变为 candidate 并发起选举的时间点通常不会相同。由于超时时间不同，首先发起选举的 candidate 很可能已经赢得了选举，其他节点在同一轮投票中无法再成为 leader。

###  raft算法中，节点根据什么规则投票?

节点在投票时遵循以下规则：

- 每个任期内只能投出一票：节点在任期内只能投给一个候选者。如果已经给一个候选者投过票，即使后来有其他候选者发出请求，节点也不会改变投票。
- 只有日志最新的候选者才能获得选票：节点会比较候选人的日志和自己的日志，确保候选人的日志不落后于自己的日志。
- 候选人的任期必须大于等于自己的任期：如果候选人的任期小于当前节点的任期，节点会拒绝投票。

## 资源

- [raft官方动画](https://raft.github.io/)
- http://arthurchiao.art/blog/raft-paper-zh/