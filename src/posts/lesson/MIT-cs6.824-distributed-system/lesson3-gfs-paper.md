---
category: 
  - 分布式系统
tag:
  - 分布式系统
---

- [谷歌分布式文件系统(GFS)](#谷歌分布式文件系统gfs)
  - [摘要](#摘要)
  - [1.介绍](#1介绍)
  - [2.设计概述](#2设计概述)
    - [2.1 假设](#21-假设)
    - [2.2 接口](#22-接口)
    - [2.3 架构](#23-架构)
    - [2.4 单个主服务器](#24-单个主服务器)
    - [2.5 块大小](#25-块大小)


# 谷歌分布式文件系统(GFS)

## 摘要

我们设计并实现了 Google 文件系统（Google File System，GFS），这是一个可扩展的分布式文件系统，旨在支持大规模分布式数据密集型应用程序。它在使用廉价的商品硬件的同时提供了容错能力，并且能够为大量客户端提供高的整体性能。

虽然它与之前的分布式文件系统在许多目标上有相似之处，但我们的设计是基于对当前和预期技术环境及应用工作负载的观察，这些观察与一些早期文件系统假设存在明显的不同。这促使我们重新审视传统的选择，并探索了与之大相径庭的设计方案。

该文件系统已成功满足我们的存储需求，并广泛部署于 Google 内部，作为我们服务中用于生成和处理数据的存储平台，同时也支持需要大量数据集的研究和开发工作。目前最大的集群提供了数百 TB 的存储，分布在数千个磁盘和千余台机器上，并且由数百个客户端同时访问。

在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了我们设计的各个方面，并报告了来自微基准测试和实际使用场景的测量结果。

## 1.介绍

我们设计并实现了谷歌文件系统（Google File System，GFS），以满足谷歌数据处理需求的快速增长。GFS 与之前的分布式文件系统有许多相同的目标，如性能、可扩展性、可靠性和可用性。然而，其设计是由对我们的应用工作负载以及当前和预期的技术环境的关键观察所驱动的，这与一些早期的文件系统设计假设明显不同。我们重新审视了传统的选择，并在设计空间中探索了截然不同的方向。

- 组件故障常态化：文件系统由大量廉价部件组成，故障不可避免，必须具备监控、容错和自动恢复能力。
- 文件大小巨大：文件通常为GB级，且数据集快速增长，管理数十亿个大文件是一个挑战，需要重新设计I/O操作和块大小。
- 文件修改方式：大多数文件通过追加数据而非随机修改，且写完之后通常是顺序读取，因此优化追加操作和保证原子性成为关键，客户端缓存不再具有优势。
- 设计应用程序与文件系统 API 的协同工作：通过放宽 GFS 的一致性模型，简化了文件系统，同时减轻了应用程序负担。引入原子追加操作使多个客户端能同时向文件追加内容，无需额外同步。

目前，多个谷歌文件系统（GFS）集群被部署用于不同的目的。最大的集群拥有超过 1000 个存储节点、超过 300TB 的磁盘存储，并且持续不断地被不同机器上的数百个客户端大量访问。

## 2.设计概述

### 2.1 假设

在为我们的需求设计文件系统时，一些既带来挑战又带来机遇的假设为我们提供了指导。我们在前面已经提及了一些关键观察结果，现在更详细地阐述我们的假设。

- 该系统由许多容易出现故障的便宜商品组件构建而成。它必须持续不断地自我监测，并在日常情况下迅速检测、容忍和从组件故障中恢复。
- 系统存储数量适中的大文件。我们预计会有几百万个文件，每个文件通常为 100MB 或更大。多 GB 的文件是常见情况，应该得到高效管理。小文件也必须得到支持，但我们无需针对小文件进行优化。
- 工作负载主要由两种读取类型组成：大型流式读取和小型随机读取。在大型流式读取中，单个操作通常读取几百 KB，更常见的是 1MB 或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小型随机读取通常在某个任意偏移处读取几 KB。注重性能的应用程序通常会对其小型读取进行批处理和排序，以便稳定地遍历文件，而不是来回读取。
- 工作负载也有许多大型的顺序写入，向文件追加数据。典型的操作大小与读取操作类似。文件一旦写入，就很少再次被修改。支持在文件中的任意位置进行小型写入，但不一定要高效。
- 系统必须为同时向同一文件追加内容的多个客户端高效地实现明确界定的语义。我们的文件通常被用作生产者消费者队列或用于多路合并。数百个生产者（每台机器上运行一个）将同时向一个文件追加内容。以最小的同步开销实现原子性是至关重要的。文件可能在以后被读取，或者一个消费者可能同时正在读取该文件。
- 高持续带宽比低延迟更重要。我们的大多数目标应用程序优先考虑以高速度批量处理数据，而很少有应用程序对单个读或写操作有严格的响应时间要求。

### 2.2 接口

GFS 提供了一个熟悉的文件系统接口，尽管它没有实现像 POSIX 这样的标准 API。文件在目录中按层次结构组织，并通过路径名进行标识。我们支持常见的创建、删除、打开、关闭、读取和写入文件的操作。

此外，GFS 具有快照和记录追加操作。快照可以以较低的成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加操作的原子性。它对于实现多路合并结果以及生产者 - 消费者队列非常有用，许多客户端可以同时向其追加数据而无需额外的锁定。我们发现这些类型的文件在构建大型分布式应用程序中非常有价值。快照和记录追加分别在第 3.4 节和第 3.3 节中进一步讨论。

### 2.3 架构

一个 GFS 集群由一个主服务器和多个块服务器组成，并由多个客户端访问，如图 1 所示。这些通常都是运行用户级服务器进程的普通 Linux 机器。只要机器资源允许，并且可以接受由于运行可能不稳定的应用程序代码而导致的较低可靠性，那么在同一台机器上同时运行块服务器和客户端是很容易的。

文件被分成固定大小的块。每个块由一个不可变且全局唯一的 64 位块句柄标识，该句柄在块创建时由主服务器分配。块服务器将块作为 Linux 文件存储在本地磁盘上，并根据块句柄和字节范围读取或写入块数据。为了可靠性，每个块在多个块服务器上进行复制。默认情况下，我们存储三个副本，不过用户可以为文件命名空间的不同区域指定不同的复制级别。

主服务器维护所有的文件系统元数据。这包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制全系统范围的活动，如块租约管理、孤立块的垃圾回收以及块服务器之间的块迁移。主服务器通过定期发送的心跳消息与每个块服务器进行通信，以给予其指令并收集其状态。

链接到每个应用程序中的 GFS 客户端代码实现文件系统 API，并代表应用程序与主服务器和块服务器进行通信以读取或写入数据。客户端与主服务器进行元数据操作交互，但所有携带数据的通信都直接指向块服务器。我们不提供 POSIX API，因此无需挂接到 Linux 的虚拟节点层。

客户端和块服务器都不缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序是流式读取巨大的文件，或者其工作集太大而无法被缓存。不进行缓存简化了客户端和整个系统，消除了缓存一致性问题。（然而，客户端确实会缓存元数据。）块服务器不需要缓存文件数据，因为块是以本地文件的形式存储的，所以 Linux 的缓冲区缓存已经将频繁访问的数据保存在内存中。

### 2.4 单个主服务器

为了简化我们的设计，我们采用了单主服务器模型。为了减少它在读取和写入中的参与，以免它成为瓶颈，客户端的交互流程如下：
- 客户端询问主服务器它应该联系哪些块服务器，并在有限的时间内缓存此信息
- 在后续操作中直接与块服务器进行交互

让我们参照图 1 解释一个简单读取操作的交互过程。

- 首先，利用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件中的块索引。然后，它向主服务器发送一个包含文件名和块索引的请求。主服务器用相应的块句柄和副本的位置进行回复。客户端以文件名和块索引作为键缓存此信息。
- 然后，客户端向其中一个副本发送请求，很可能是最近的一个。请求指定块句柄和该块内的字节范围。对同一块的进一步读取在缓存信息过期或文件重新打开之前不需要更多的客户端 - 主服务器交互。实际上，客户端通常在同一个请求中请求多个块，并且主服务器还可以包括紧接在请求的块之后的那些块的信息。这些额外信息几乎无需额外成本就避免了未来的几次客户端 - 主服务器交互。

### 2.5 块大小

块大小是关键设计参数之一。我们选择了 64 兆字节（64 MB），这比典型的文件系统块大小要大得多。每个块副本在块服务器上作为一个普通的 Linux 文件存储，并且仅在需要时进行扩展。

惰性空间分配避免了因内部碎片而造成的空间浪费，内部碎片问题可能是人们对如此大的块大小最主要的反对理由。

较大的块大小具有几个重要优势。
- 首先，它减少了客户端与主服务器交互的需求，因为对同一个块的读写操作只需向主服务器发起一次初始请求以获取块位置信息。由于应用程序大多是按顺序读写大文件，所以这种减少对于我们的工作负载来说尤为显著。即使是对于小的随机读取，客户端也可以轻松地缓存数太字节（TB）工作集的所有块位置信息。
- 其次，由于在一个大块上，客户端更有可能对给定的块执行许多操作，因此它可以通过在较长时间段内与块服务器保持持久的 TCP 连接来减少网络开销。
- 第三，它减小了存储在主服务器上的元数据的大小。这使得我们能够将元数据保存在内存中，进而带来我们将在 2.6.1 节讨论的其他优势。

另一方面，即使采用了惰性空间分配，较大的块大小也有其劣势。一个小文件由少量的块组成，可能只有一个块。如果有许多客户端访问同一个文件，存储这些块的块服务器可能会成为热点。实际上，热点问题并不是一个主要问题，因为我们的应用程序大多是按顺序读取由多个块组成的大文件。

然而，当批处理队列系统首次使用谷歌文件系统（GFS）时，确实出现了热点问题：一个可执行文件作为单块文件写入 GFS，然后在同一时间在数百台机器上启动。存储这个可执行文件的少数几块服务器因数百个同时发出的请求而超载。我们通过以更高的复制因子存储此类可执行文件以及让批处理队列系统错开应用程序启动时间来解决了这个问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。

这里的"惰性分配"（lazy allocation）指的是在实际需要时才分配存储空间，而不是预先为每个块分配完整的64 MB空间。换句话说，当块大小设置为64 MB时，系统不会立即分配整个64 MB的磁盘空间，而是等到数据真正写入时才逐步扩展块的大小，直到达到64 MB的上限。